{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363732af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow_addons\n",
    "!pip install -q pyarrow\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66841318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 06:18:40.818963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 06:18:41.439375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/tensorflow/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sn\n",
    "import datetime\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "import sklearn\n",
    "import scipy\n",
    "import io\n",
    "import json\n",
    "\n",
    "# import wandb\n",
    "\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd110003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Setup Weights and Biases\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15ec671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOG_DIR = './logs/fit'\n",
    "# wandb.tensorboard.patch(root_logdir= LOG_DIR)\n",
    "# wandb.init(project='w251-GISLR-Final', \n",
    "#            config=config,\n",
    "#           sync_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1306e",
   "metadata": {
    "papermill": {
     "duration": 0.014594,
     "end_time": "2023-03-27T13:49:16.606639",
     "exception": false,
     "start_time": "2023-03-27T13:49:16.592045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1110cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \n",
    "    \"N_COLS0\": 543,\n",
    "    \"N_COLS\": 227,\n",
    "    \"N_ROWS\":543,\n",
    "    \"N_DIMS\": 2, \n",
    "    \"N_EPOCHS\": 150,\n",
    "    \"TRAIN_BATCH_SIZE\": 128,\n",
    "    \"INPUT_SIZE\":38,\n",
    "    \"BATCH_ALL_SIGNS_N\": 4,\n",
    "    \"NUM_CLASSES\": 250,\n",
    "    \"BATCH_SIZE_VAL\":128,\n",
    "    \"WD_RATIO\":0.05,\n",
    "    \"LEARNING_RATE\": 0.01,\n",
    "    \"WEIGHT_DECAY\": 0.00001,\n",
    "    \"N_WARMUP_EPOCHS\": 0,\n",
    "    \"MASK_VAL\": 4237\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f840f81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/competitions/asl-signs/overview/evaluation\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "#w251-asl-data/raw-data/train_landmark_files/28656/3311214787.parquet\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc095c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prints Shape and Dtype For List Of Variables\n",
    "def print_shape_dtype(l, names):\n",
    "    for e, n in zip(l, names):\n",
    "        print(f'{n} shape: {e.shape}, dtype: {e.dtype}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d55e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "class PreprocessLayerV2(tf.keras.layers.Layer):\n",
    "    def __init__(self, INPUT_SIZE):\n",
    "        super(PreprocessLayerV2, self).__init__()\n",
    "        self.INPUT_SIZE = INPUT_SIZE\n",
    "        # Indicies in original data. \n",
    "        self.FACE_IDXS = tf.constant([0, 6, 7, 11, 12, 13, 14, 15, 17, 22, 23, 24, 25, 26, 30, 31, \n",
    "                     33, 37, 38, 39, 40, 41, 42, 56, 61, 62, 72, 73, 74, 76, 77, \n",
    "                     78, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 95, 96, 110, 112, \n",
    "                     113, 122, 128, 130, 133, 144, 145, 146, 153, 154, 155, 157, 158, \n",
    "                     159, 160, 161, 163, 168, 173, 178, 179, 180, 181, 183, 184, 185, \n",
    "                     188, 189, 190, 191, 193, 196, 197, 232, 233, 243, 244, 245, 246, \n",
    "                     247, 249, 252, 253, 254, 255, 256, 259, 260, 263, 267, 268, 269, \n",
    "                     270, 271, 272, 286, 291, 292, 302, 303, 304, 306, 307, 308, 310, \n",
    "                     311, 312, 314, 316, 317, 318, 319, 320, 321, 324, 325, 339, 341, \n",
    "                     351, 357, 359, 362, 373, 374, 375, 380, 381, 382, 384, 385, 386, \n",
    "                     387, 388, 390, 398, 402, 403, 404, 405, 407, 408, 409, 412, 413, \n",
    "                     414, 415, 417, 419, 453, 463, 464, 465, 466, 467], dtype=tf.int32)\n",
    "        self.POSE_IDXS = tf.constant(tf.range(489, 514, delta=1, dtype=tf.int32))\n",
    "        self.LEFT_HAND_IDXS = tf.constant(tf.range(468, 489, delta=1, dtype=tf.int32))\n",
    "        self.RIGHT_HAND_IDXS = tf.constant(tf.range(522, 543, delta=1, dtype=tf.int32))\n",
    "        \n",
    "        self.HAND_IDXS = tf.constant(tf.concat([self.LEFT_HAND_IDXS, self.RIGHT_HAND_IDXS], 0), dtype=tf.int32)\n",
    "            \n",
    "        # All landmarks that are used for modeling. \n",
    "        self.LANDMARK_IDXS = tf.constant(tf.concat([self.FACE_IDXS, self.POSE_IDXS, self.LEFT_HAND_IDXS, self.RIGHT_HAND_IDXS], 0), dtype=tf.int32)\n",
    "        \n",
    "        # Indicies after landmarks have been filtered. \n",
    "        self.FACE_START = tf.constant(0, dtype=tf.int32)\n",
    "        self.LEFT_HAND_START = tf.constant(len(self.FACE_IDXS), dtype=tf.int32)\n",
    "        self.POSE_START = tf.constant(self.LEFT_HAND_START + len(self.LEFT_HAND_IDXS), dtype=tf.int32)\n",
    "        self.RIGHT_HAND_START = tf.constant(self.POSE_START + len(self.POSE_IDXS), dtype=tf.int32)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'INPUT_SIZE': self.INPUT_SIZE,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    \n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[None, 543, 2], dtype=tf.float32),),)\n",
    "    def call(self, data):\n",
    "        \n",
    "        # Filter Out Frames With Empty Hand Data\n",
    "        frames_hands_nansum = tf.experimental.numpy.nanmean(tf.gather(data, self.HAND_IDXS, axis=1), axis=[1,2])\n",
    "        non_empty_frames_idxs = tf.where(frames_hands_nansum > 0)\n",
    "        non_empty_frames_idxs = tf.squeeze(non_empty_frames_idxs, axis=1)\n",
    "        data = tf.gather(data, non_empty_frames_idxs, axis=0)\n",
    "        \n",
    "        # Cast Indices in float32 to be compatible with Tensorflow Lite\n",
    "        non_empty_frames_idxs = tf.cast(non_empty_frames_idxs, tf.float32) \n",
    "        \n",
    "        N_FRAMES = tf.shape(data)[0]\n",
    "        data = tf.gather(data, self.LANDMARK_IDXS, axis=1)\n",
    "        \n",
    "        # Slice out face indicies, normalize across batch.        \n",
    "        face = tf.slice(data, [0, self.FACE_START, 0], [N_FRAMES, self.LEFT_HAND_START, 2])\n",
    "        # face_mean, face_std = self.get_mean_std(self.FACE_IDXS, face)\n",
    "        xs = tf.reshape(tf.transpose(face, [2,0,1]), [2,tf.size(self.FACE_IDXS)*tf.shape(face)[0]])[0]\n",
    "        ys = tf.reshape(tf.transpose(face, [2,0,1]), [2,tf.size(self.FACE_IDXS)*tf.shape(face)[0]])[1]\n",
    "            \n",
    "        FACE_MEAN_X = tf.math.reduce_mean(xs)\n",
    "        FACE_STD_X = tf.math.reduce_std(xs)\n",
    "        FACE_MEAN_Y = tf.math.reduce_mean(ys)\n",
    "        FACE_STD_Y = tf.math.reduce_std(ys)\n",
    "\n",
    "        face_mean = tf.stack([FACE_MEAN_X, FACE_MEAN_Y])\n",
    "        face_std = tf.stack([FACE_STD_X, FACE_STD_Y])\n",
    "        face = tf.where(\n",
    "                    tf.math.equal(face, 0.0),\n",
    "                    0.0,\n",
    "                    (face - face_mean) / face_std,\n",
    "                )\n",
    "        \n",
    "#         # Slice out left_hand indicies, normalize across batch.\n",
    "        left_hand = tf.slice(data, [0, self.LEFT_HAND_START, 0], [N_FRAMES, self.POSE_START-self.LEFT_HAND_START, 2])\n",
    "        xs = tf.reshape(tf.transpose(left_hand, [2,0,1]), [2,tf.size(self.LEFT_HAND_IDXS)*tf.shape(left_hand)[0]])[0]\n",
    "        ys = tf.reshape(tf.transpose(left_hand, [2,0,1]), [2,tf.size(self.LEFT_HAND_IDXS)*tf.shape(left_hand)[0]])[1]\n",
    "            \n",
    "        LEFT_HAND_MEAN_X = tf.math.reduce_mean(xs)\n",
    "        LEFT_HAND_STD_X = tf.math.reduce_std(xs)\n",
    "        LEFT_HAND_MEAN_Y = tf.math.reduce_mean(ys)\n",
    "        LEFT_HAND_STD_Y = tf.math.reduce_std(ys)\n",
    "\n",
    "        left_hand_mean = tf.stack([LEFT_HAND_MEAN_X, LEFT_HAND_MEAN_Y])\n",
    "        left_hand_std = tf.stack([LEFT_HAND_STD_X, LEFT_HAND_STD_Y])\n",
    "        left_hand = tf.where(\n",
    "                    tf.math.equal(left_hand, 0.0),\n",
    "                    0.0,\n",
    "                    (left_hand - left_hand_mean) / left_hand_std,\n",
    "                )\n",
    "        \n",
    "#         # Slice out pose indicies, normalize across batch.\n",
    "        pose = tf.slice(data, [0, self.POSE_START, 0], [N_FRAMES, self.RIGHT_HAND_START-self.POSE_START, 2])\n",
    "        xs = tf.reshape(tf.transpose(pose, [2,0,1]), [2,tf.size(self.POSE_IDXS)*tf.shape(pose)[0]])[0]\n",
    "        ys = tf.reshape(tf.transpose(pose, [2,0,1]), [2,tf.size(self.POSE_IDXS)*tf.shape(pose)[0]])[1]\n",
    "            \n",
    "        POSE_MEAN_X = tf.math.reduce_mean(xs)\n",
    "        POSE_STD_X = tf.math.reduce_std(xs)\n",
    "        POSE_MEAN_Y = tf.math.reduce_mean(ys)\n",
    "        POSE_STD_Y = tf.math.reduce_std(ys)\n",
    "\n",
    "        pose_mean = tf.stack([POSE_MEAN_X, POSE_MEAN_Y])\n",
    "        pose_std = tf.stack([POSE_STD_X, POSE_STD_Y])\n",
    "        pose = tf.where(\n",
    "                    tf.math.equal(pose, 0.0),\n",
    "                    0.0,\n",
    "                    (pose - pose_mean) / pose_std,\n",
    "                )\n",
    "        \n",
    "         # Slice out right_hand indicies, normalize across batch.\n",
    "        right_hand = tf.slice(data, [0, self.RIGHT_HAND_START, 0], [N_FRAMES, tf.shape(data)[1] - self.RIGHT_HAND_START, 2])\n",
    "        xs = tf.reshape(tf.transpose(right_hand, [2,0,1]), [2,tf.size(self.LEFT_HAND_IDXS)*tf.shape(right_hand)[0]])[0]\n",
    "        ys = tf.reshape(tf.transpose(right_hand, [2,0,1]), [2,tf.size(self.LEFT_HAND_IDXS)*tf.shape(right_hand)[0]])[1]\n",
    "            \n",
    "        RIGHT_HAND_MEAN_X = tf.math.reduce_mean(xs)\n",
    "        RIGHT_HAND_STD_X = tf.math.reduce_std(xs)\n",
    "        RIGHT_HAND_MEAN_Y = tf.math.reduce_mean(ys)\n",
    "        RIGHT_HAND_STD_Y = tf.math.reduce_std(ys)\n",
    "\n",
    "        right_hand_mean = tf.stack([RIGHT_HAND_MEAN_X, RIGHT_HAND_MEAN_Y])\n",
    "        right_hand_std = tf.stack([RIGHT_HAND_STD_X, RIGHT_HAND_STD_Y])\n",
    "        right_hand = tf.where(\n",
    "                    tf.math.equal(right_hand, 0.0),\n",
    "                    0.0,\n",
    "                    (right_hand - right_hand_mean) / right_hand_std,\n",
    "                )\n",
    "        \n",
    "        \n",
    "        # Concat landmarks back into same frame.\n",
    "        data = tf.concat([face, left_hand, pose, right_hand], 1)\n",
    "        \n",
    "        \n",
    "        # Video fits in self.INPUT_SIZE\n",
    "        if N_FRAMES < self.INPUT_SIZE: # Number of frames we want\n",
    "            # Attention mask for frames that contain data. \n",
    "            \n",
    "            non_empty_frames_idxs = tf.pad(non_empty_frames_idxs, [[0, self.INPUT_SIZE-N_FRAMES]], constant_values=-1)\n",
    "            data = tf.pad(data, [[0, self.INPUT_SIZE-N_FRAMES], [0,0], [0,0]], constant_values=0)\n",
    "            # Fill NaN Values With 0\n",
    "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
    "            # Reshape into (Number of desired frames, (Number of landmarks * 2))\n",
    "            data = tf.reshape(data, [self.INPUT_SIZE, tf.shape(data)[1] * 2])\n",
    "            return data, non_empty_frames_idxs\n",
    "        # Video needs to be downsampled to INPUT_SIZE\n",
    "        else:\n",
    "            # Downsample video using nearest interpolation method. \n",
    "            data = tf.image.resize(data, size=(self.INPUT_SIZE, data.shape[1]), method='nearest')\n",
    "            # Fill NaN Values With 0\n",
    "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
    "            # Reshape into (Number of desired frames, (Number of landmarks * 2)).\n",
    "            data = tf.reshape(data, [self.INPUT_SIZE, tf.shape(data)[1] * 2])\n",
    "            # Create attention mask with all frames. \n",
    "            non_empty_frames_idxs = tf.range(0, self.INPUT_SIZE, 1, dtype=tf.float32)\n",
    "            return data, non_empty_frames_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d982f07e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_SAMPLES: 94477\n"
     ]
    }
   ],
   "source": [
    "train_metadata = pd.read_csv('train.csv')\n",
    "\n",
    "N_SAMPLES = len(train_metadata)\n",
    "print(f'N_SAMPLES: {N_SAMPLES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b8b57f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get complete file path to file\n",
    "def get_file_path(path):\n",
    "    return f'/kaggle/input/asl-signs/{path}'\n",
    "\n",
    "train_metadata['file_path'] = train_metadata['path'].apply(get_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c2926ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add ordinally Encoded Sign (assign number to each sign name)\n",
    "train_metadata['sign_ord'] = train_metadata['sign'].astype('category').cat.codes\n",
    "\n",
    "# Dictionaries to translate sign <-> ordinal encoded sign\n",
    "SIGN2ORD = train_metadata[['sign', 'sign_ord']].set_index('sign').squeeze().to_dict()\n",
    "ORD2SIGN = train_metadata[['sign_ord', 'sign']].set_index('sign_ord').squeeze().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e35f25f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 06:18:43.210161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.230003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.232092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.235455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.237381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.239125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.739804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.740802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.741546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-19 06:18:43.742278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20561 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "preprocess_layer = PreprocessLayerV2(config[\"INPUT_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66850e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    face: 0:468\n",
    "    left_hand: 468:489\n",
    "    pose: 489:522\n",
    "    right_hand: 522:544\n",
    "        \n",
    "\"\"\"\n",
    "def get_data(file_path):\n",
    "    # Load Raw Data\n",
    "    data = load_relevant_data_subset(file_path)\n",
    "    # Process Data Using Tensorflow\n",
    "    data = preprocess_layer(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d43c6fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the full dataset\n",
    "def preprocess_dataset():\n",
    "    # Create arrays to save data\n",
    "    X = np.zeros([N_SAMPLES, config[\"INPUT_SIZE\"], config[\"N_COLS\"] * config[\"N_DIMS\"]], dtype=np.float32)\n",
    "    y = np.zeros([N_SAMPLES], dtype=np.int32)\n",
    "    NON_EMPTY_FRAME_IDXS = np.full([N_SAMPLES, config[\"INPUT_SIZE\"]], -1, dtype=np.float32)\n",
    "\n",
    "    for row_idx, (file_path, sign_ord) in enumerate(tqdm(train_metadata[['file_path', 'sign_ord']].values)):\n",
    "        if row_idx % 5000 == 0:\n",
    "            print(f'Generated {row_idx}/{N_SAMPLES}')\n",
    "\n",
    "        data, non_empty_frame_idxs = get_data(file_path)\n",
    "        X[row_idx] = data\n",
    "        y[row_idx] = sign_ord\n",
    "        NON_EMPTY_FRAME_IDXS[row_idx] = non_empty_frame_idxs\n",
    "        if np.isnan(data).sum() > 0:\n",
    "            print(row_idx)\n",
    "            return data\n",
    "    \n",
    "    # Save X/y\n",
    "    np.save('X.npy', X)\n",
    "    np.save('y.npy', y)\n",
    "    np.save('NON_EMPTY_FRAME_IDXS.npy', NON_EMPTY_FRAME_IDXS)\n",
    "        \n",
    "    return X, y, NON_EMPTY_FRAME_IDXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e191aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X, y, NON_EMPTY_FRAME_IDXS = preprocess_dataset()\n",
    "\n",
    "# print_shape_dtype([X, y, NON_EMPTY_FRAME_IDXS], ['X', 'y', 'NON_EMPTY_FRAME_IDXS'])\n",
    "# print(f'# NaN Values X: {np.isnan(X).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50d3d8db",
   "metadata": {
    "papermill": {
     "duration": 0.039339,
     "end_time": "2023-03-27T13:50:42.441202",
     "exception": false,
     "start_time": "2023-03-27T13:50:42.401863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_VAL = True\n",
    "\n",
    "DIM_NAMES = ['x', 'y']\n",
    "SEED = 42\n",
    "\n",
    "TRANSFORMERV1 = True\n",
    "\n",
    "# Epsilon value for layer normalisation\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "\n",
    "# Dense layer units for landmarks\n",
    "FACE_UNITS = 384\n",
    "HANDS_UNITS = 384\n",
    "POSE_UNITS = 384\n",
    "# final embedding and transformer embedding size\n",
    "UNITS = 512\n",
    "\n",
    "# Transformer\n",
    "NUM_BLOCKS = 4\n",
    "MLP_RATIO = 2\n",
    "NUM_HEADS = 4\n",
    "\n",
    "# Dropout\n",
    "EMBEDDING_DROPOUT = 0.0\n",
    "MLP_DROPOUT_RATIO = 0.1\n",
    "CLASSIFIER_DROPOUT_RATIO = 0.1\n",
    "\n",
    "# Initiailizers\n",
    "# INIT_HE_UNIFORM = tf.keras.initializers.he_uniform\n",
    "INIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\n",
    "# INIT_ZEROS = tf.keras.initializers.constant(0.0)\n",
    "# Activations\n",
    "ACTIVATION = tf.keras.activations.gelu\n",
    "\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec5d4732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FACE_IDXS = [0, 6, 7, 11, 12, 13, 14, 15, 17, 22, 23, 24, 25, 26, 30, 31, \n",
    "                     33, 37, 38, 39, 40, 41, 42, 56, 61, 62, 72, 73, 74, 76, 77, \n",
    "                     78, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 95, 96, 110, 112, \n",
    "                     113, 122, 128, 130, 133, 144, 145, 146, 153, 154, 155, 157, 158, \n",
    "                     159, 160, 161, 163, 168, 173, 178, 179, 180, 181, 183, 184, 185, \n",
    "                     188, 189, 190, 191, 193, 196, 197, 232, 233, 243, 244, 245, 246, \n",
    "                     247, 249, 252, 253, 254, 255, 256, 259, 260, 263, 267, 268, 269, \n",
    "                     270, 271, 272, 286, 291, 292, 302, 303, 304, 306, 307, 308, 310, \n",
    "                     311, 312, 314, 316, 317, 318, 319, 320, 321, 324, 325, 339, 341, \n",
    "                     351, 357, 359, 362, 373, 374, 375, 380, 381, 382, 384, 385, 386, \n",
    "                     387, 388, 390, 398, 402, 403, 404, 405, 407, 408, 409, 412, 413, \n",
    "                     414, 415, 417, 419, 453, 463, 464, 465, 466, 467]\n",
    "POSE_IDXS = np.arange(489, 514)\n",
    "LEFT_HAND_IDXS = np.arange(468, 489)\n",
    "RIGHT_HAND_IDXS = np.arange(522, 543)\n",
    "\n",
    "# All landmarks that are used for modeling. \n",
    "LANDMARK_IDXS = np.concatenate((FACE_IDXS, POSE_IDXS, LEFT_HAND_IDXS, RIGHT_HAND_IDXS))\n",
    "\n",
    "# Indicies after landmarks have been filtered. \n",
    "FACE_START = 0\n",
    "LEFT_HAND_START = len(FACE_IDXS)\n",
    "POSE_START = LEFT_HAND_START + len(LEFT_HAND_IDXS)\n",
    "RIGHT_HAND_START = POSE_START + len(POSE_IDXS)\n",
    "\n",
    "# Length of landmarks.\n",
    "FACE_LEN = len(FACE_IDXS)\n",
    "POSE_LEN = POSE_IDXS.size\n",
    "LEFT_HAND_LEN = LEFT_HAND_IDXS.size\n",
    "RIGHT_HAND_LEN = RIGHT_HAND_IDXS.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7838ebb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LandmarkEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, name, activation):\n",
    "        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
    "        self.UNITS = units\n",
    "        self.ACTIVATION = activation\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'UNITS': self.UNITS,\n",
    "            'ACTIVATION': self.ACTIVATION,\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Embedding for missing landmark in frame, initizlied with zeros\n",
    "        self.empty_embedding = self.add_weight(\n",
    "            name=f'{self.name}_empty_embedding',\n",
    "            shape=[self.UNITS],\n",
    "            initializer=tf.keras.initializers.constant(0.0),\n",
    "        )\n",
    "        # Embedding\n",
    "        self.dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.UNITS, name=f'{self.name}_dense_1', use_bias=False, \n",
    "                                  kernel_initializer=tf.keras.initializers.glorot_uniform, activation=self.ACTIVATION),\n",
    "            tf.keras.layers.Dense(self.UNITS, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=tf.keras.initializers.he_uniform),\n",
    "        ], name=f'{self.name}_dense')\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.where(\n",
    "                # Checks whether landmark is missing in frame\n",
    "                tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
    "                # If so, the empty embedding is used\n",
    "                self.empty_embedding,\n",
    "                # Otherwise the landmark data is embedded\n",
    "                self.dense(x),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bb44e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_size, face_units, hands_units, pose_units, units, activation):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.INPUT_SIZE = input_size\n",
    "        self.FACE_UNITS = face_units\n",
    "        self.HANDS_UNITS = hands_units\n",
    "        self.POSE_UNITS = pose_units\n",
    "        self.UNITS = units\n",
    "        self.ACTIVATION = activation\n",
    "        \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'INPUT_SIZE': self.INPUT_SIZE,\n",
    "            'FACE_UNITS': self.FACE_UNITS,\n",
    "            'HANDS_UNITS': self.HANDS_UNITS,\n",
    "            'POSE_UNITS': self.POSE_UNITS,\n",
    "            'UNITS': self.UNITS,\n",
    "            'ACTIVATION': self.ACTIVATION,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Positional Embedding, initialized with zeros\n",
    "        self.positional_embedding = tf.keras.layers.Embedding(self.INPUT_SIZE+1, self.UNITS, embeddings_initializer=tf.keras.initializers.constant(0.0))\n",
    "        # Embedding layer for Landmarks\n",
    "        self.face_embedding = LandmarkEmbedding(self.FACE_UNITS, 'face', self.ACTIVATION)\n",
    "        self.left_hand_embedding = LandmarkEmbedding(self.HANDS_UNITS, 'left_hand', self.ACTIVATION)\n",
    "        self.right_hand_embedding = LandmarkEmbedding(self.HANDS_UNITS, 'right_hand', self.ACTIVATION)\n",
    "        self.pose_embedding = LandmarkEmbedding(self.POSE_UNITS, 'pose', self.ACTIVATION)\n",
    "        # Landmark Weights\n",
    "        self.landmark_weights = tf.Variable(tf.zeros([4], dtype=tf.float32), name='landmark_weights')\n",
    "        # Fully Connected Layers for combined landmarks\n",
    "        self.fc = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.UNITS, name='fully_connected_1', use_bias=False, \n",
    "                                  kernel_initializer=tf.keras.initializers.glorot_uniform, activation=self.ACTIVATION),\n",
    "            tf.keras.layers.Dense(self.UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=tf.keras.initializers.he_uniform),\n",
    "        ], name='fc')\n",
    "\n",
    "\n",
    "    def call(self, face0, left_hand0, right_hand0, pose0, non_empty_frame_idxs,training=False):\n",
    "        # Face\n",
    "        face_embedding = self.face_embedding(face0)\n",
    "        # Left Hand\n",
    "        left_hand_embedding = self.left_hand_embedding(left_hand0)\n",
    "        # Right Hand\n",
    "        right_hand_embedding = self.right_hand_embedding(right_hand0)\n",
    "        # Pose\n",
    "        pose_embedding = self.pose_embedding(pose0)\n",
    "        # Merge Embeddings of all landmarks with mean pooling\n",
    "        x = tf.stack((face_embedding, left_hand_embedding, right_hand_embedding, pose_embedding), axis=3)\n",
    "        # Merge Landmarks with trainable attention weights\n",
    "        x = x * tf.nn.softmax(self.landmark_weights)\n",
    "        x = tf.reduce_sum(x, axis=3)\n",
    "        # Fully Connected Layers\n",
    "        x = self.fc(x)\n",
    "        # Add Positional Embedding\n",
    "        normalised_non_empty_frame_idxs = tf.where(\n",
    "            tf.math.equal(non_empty_frame_idxs, -1.0),\n",
    "            self.INPUT_SIZE,\n",
    "            tf.cast(\n",
    "                non_empty_frame_idxs / tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True) * self.INPUT_SIZE,\n",
    "                tf.int32,\n",
    "            ),\n",
    "        )\n",
    "        x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af24773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# based on: https://stackoverflow.com/questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n",
    "# replaced softmax with softmax layer to support masked softmax\n",
    "def scaled_dot_product(q,k,v, softmax, attention_mask):\n",
    "    #calculates Q . K(transpose)\n",
    "    qkt = tf.matmul(q,k,transpose_b=True)\n",
    "    #caculates scaling factor\n",
    "    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n",
    "    scaled_qkt = qkt/dk\n",
    "    softmax = softmax(scaled_qkt, mask=attention_mask)\n",
    "    \n",
    "    z = tf.matmul(softmax,v)\n",
    "    #shape: (m,Tx,depth), same shape as q,k,v\n",
    "    return z\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_of_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.depth = d_model//num_of_heads\n",
    "        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wo = tf.keras.layers.Dense(d_model)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_of_heads': self.num_of_heads\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    def call(self,x, attention_mask):\n",
    "        multi_attn = []\n",
    "        for i in range(self.num_of_heads):\n",
    "            Q = self.wq[i](x)\n",
    "            K = self.wk[i](x)\n",
    "            V = self.wv[i](x)\n",
    "            multi_attn.append(scaled_dot_product(Q,K,V, self.softmax, attention_mask))\n",
    "            \n",
    "        multi_head = tf.concat(multi_attn,axis=-1)\n",
    "        multi_head_attention = self.wo(multi_head)\n",
    "        return multi_head_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8bcc760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full Transformer\n",
    "class Transformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_blocks, layer_norm_eps, units, mlp_ratio, mlp_dropout_ratio, activation):\n",
    "        super(Transformer, self).__init__(name='transformer')\n",
    "        self.NUM_BLOCKS = num_blocks\n",
    "        self.LAYER_NORM_EPS = layer_norm_eps\n",
    "        self.UNITS = units\n",
    "        self.MLP_RATIO = mlp_ratio\n",
    "        self.MLP_DROPOUT_RATIO = mlp_dropout_ratio\n",
    "        self.ACTIVATION = activation\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'NUM_BLOCKS': self.NUM_BLOCKS,\n",
    "            'LAYER_NORM_EPS': self.LAYER_NORM_EPS,\n",
    "            'MLP_RATIO': self.MLP_RATIO,\n",
    "            'MLP_DROPOUT_RATIO': self.MLP_DROPOUT_RATIO,\n",
    "            'UNITS': self.UNITS,\n",
    "            'ACTIVATION': self.ACTIVATION,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.ln_1s = []\n",
    "        self.mhas = []\n",
    "        self.ln_2s = []\n",
    "        self.mlps = []\n",
    "        # Make Transformer Blocks\n",
    "        for i in range(self.NUM_BLOCKS):\n",
    "            # First Layer Normalisation\n",
    "            self.ln_1s.append(tf.keras.layers.LayerNormalization(epsilon=self.LAYER_NORM_EPS))\n",
    "            # Multi Head Attention\n",
    "            self.mhas.append(MultiHeadAttention(self.UNITS, 4))\n",
    "            # Second Layer Normalisation\n",
    "            self.ln_2s.append(tf.keras.layers.LayerNormalization(epsilon=self.LAYER_NORM_EPS))\n",
    "            # Multi Layer Perception\n",
    "            self.mlps.append(tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(self.UNITS * self.MLP_RATIO, activation=self.ACTIVATION, \n",
    "                                      kernel_initializer=tf.keras.initializers.glorot_uniform),\n",
    "                tf.keras.layers.Dropout(self.MLP_DROPOUT_RATIO),\n",
    "                tf.keras.layers.Dense(self.UNITS, kernel_initializer=tf.keras.initializers.he_uniform),\n",
    "            ]))\n",
    "        \n",
    "    def call(self, x, attention_mask):\n",
    "        # Iterate input over transformer blocks\n",
    "        for ln_1, mha, ln_2, mlp in zip(self.ln_1s, self.mhas, self.ln_2s, self.mlps):\n",
    "            x1 = ln_1(x)\n",
    "            attention_output = mha(x1, attention_mask)\n",
    "            x2 = x1 + attention_output\n",
    "            x3 = ln_2(x2)\n",
    "            x3 = mlp(x3)\n",
    "            x = x3 + x2\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec8d504c",
   "metadata": {
    "papermill": {
     "duration": 0.043124,
     "end_time": "2023-03-27T13:50:42.896486",
     "exception": false,
     "start_time": "2023-03-27T13:50:42.853362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # Inputs\n",
    "    frames = tf.keras.layers.Input([config[\"INPUT_SIZE\"], config[\"N_COLS\"] * config[\"N_DIMS\"]], dtype=tf.float32, name='FRAMES')\n",
    "    non_empty_frame_idxs = tf.keras.layers.Input([config[\"INPUT_SIZE\"]], dtype=tf.float32, name='NON_EMPTY_FRAME_IDXS')\n",
    "    \n",
    "    # Attention Mask\n",
    "    mask = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=2)\n",
    "    \n",
    "    # Slice out face indicies       \n",
    "    face = tf.slice(frames, [0, 0, FACE_START], [-1, config[\"INPUT_SIZE\"], FACE_LEN * 2])\n",
    "    # face = tf.reshape(frames, [-1, config[\"INPUT_SIZE\"], FACE_LEN*2])\n",
    "    \n",
    "     # Slice out left_hand indicies\n",
    "    left_hand = tf.slice(frames, [0, 0, LEFT_HAND_START * 2], [-1, config[\"INPUT_SIZE\"], LEFT_HAND_LEN * 2])\n",
    "    # left_hand = tf.reshape(frames, [-1, config[\"INPUT_SIZE\"], len(LEFT_HAND_IDXS)*2])\n",
    "\n",
    "    # Slice out pose indicies\n",
    "    pose = tf.slice(frames, [0, 0, POSE_START * 2], [-1, config[\"INPUT_SIZE\"], POSE_LEN * 2])\n",
    "    # pose = tf.reshape(frames, [-1, config[\"INPUT_SIZE\"], len(POSE_IDXS)*2])\n",
    "\n",
    "    # Slice out right_hand indicies\n",
    "    right_hand = tf.slice(frames, [0, 0, RIGHT_HAND_START * 2], [-1, config[\"INPUT_SIZE\"], RIGHT_HAND_LEN * 2])\n",
    "    # right_hand = tf.reshape(frames, [-1, config[\"INPUT_SIZE\"], len(RIGHT_HAND_IDXS)*2])\n",
    "    \n",
    "    embedding_layer = Embedding(config[\"INPUT_SIZE\"], FACE_UNITS, HANDS_UNITS, POSE_UNITS, UNITS, ACTIVATION)\n",
    "    x = embedding_layer(face, left_hand, right_hand, pose, non_empty_frame_idxs)\n",
    "    transformer_input_shape = x.shape\n",
    "    \n",
    "    if (TRANSFORMERV1):\n",
    "        # Encoder Transformer Blocks\n",
    "        transformer_layer = Transformer(NUM_BLOCKS, LAYER_NORM_EPS, UNITS, MLP_RATIO, MLP_DROPOUT_RATIO, ACTIVATION)\n",
    "        x = transformer_layer(x, mask)\n",
    "    else:\n",
    "        encoder_input_shape = transformer_input_shape\n",
    "        for _ in range(NUM_BLOCKS):\n",
    "            x = TransformerV2(encoder_input_shape, NUM_HEADS, UNITS, MLP_DROPOUT_RATIO, LAYER_NORM_EPS)(x)\n",
    "            encoder_input_shape = x.shape[1:]  # Update the input shape for the next encoder\n",
    "    \n",
    "    # Pooling\n",
    "    x = tf.reduce_sum(x * mask, axis=1) / tf.reduce_sum(mask, axis=1)\n",
    "    # Classifier Dropout\n",
    "    x = tf.keras.layers.Dropout(CLASSIFIER_DROPOUT_RATIO)(x)\n",
    "    # Classification Layer\n",
    "    x = tf.keras.layers.Dense(config[\"NUM_CLASSES\"], activation=tf.keras.activations.softmax, kernel_initializer=INIT_GLOROT_UNIFORM)(x)\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    # Create Tensorflow Model\n",
    "    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n",
    "    \n",
    "    # Simple Categorical Crossentropy Loss\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    # Adam Optimizer with weight decay\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=config[\"LEARNING_RATE\"], weight_decay=config[\"WEIGHT_DECAY\"])\n",
    "    \n",
    "    # TopK Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n",
    "    ]\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics, run_eagerly=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c752db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')\n",
    "NON_EMPTY_FRAME_IDXS = np.load('NON_EMPTY_FRAME_IDXS.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "841f78e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom sampler to get a batch containing N times all signs\n",
    "def get_train_batch(X, y, NON_EMPTY_FRAME_IDXS, batch_size=128):\n",
    "    # Arrays to store batch in\n",
    "    X_batch = np.zeros([batch_size, config['INPUT_SIZE'], config['N_COLS'] * config['N_DIMS']], dtype=np.float32)\n",
    "    y_batch = np.arange(0, batch_size, step=1/batch_size, dtype=np.float32).astype(np.int64)\n",
    "    non_empty_frame_idxs_batch = np.zeros([batch_size, config['INPUT_SIZE']], dtype=np.float32)\n",
    "    \n",
    "    samples = X.shape[0]\n",
    "    n_batches = samples // batch_size\n",
    "    \n",
    "    while True:\n",
    "        for index in range(n_batches):\n",
    "\n",
    "            X_batch = X[(index*batch_size):((index+1)*batch_size)]\n",
    "            y_batch = y[(index*batch_size):((index+1)*batch_size)]\n",
    "            NON_EMPTY_FRAME_IDXS_batch = NON_EMPTY_FRAME_IDXS[(index*batch_size):((index+1)*batch_size)]\n",
    "\n",
    "        yield { 'FRAMES': X_batch, 'NON_EMPTY_FRAME_IDXS': NON_EMPTY_FRAME_IDXS_batch }, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "494d2293-9bf9-4b42-9fc7-03eaed798c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "NON_EMPTY_FRAME_IDXS_train, NON_EMPTY_FRAME_IDXS_val = train_test_split(NON_EMPTY_FRAME_IDXS, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779db5cb",
   "metadata": {
    "papermill": {
     "duration": 0.038802,
     "end_time": "2023-03-27T13:51:03.381634",
     "exception": false,
     "start_time": "2023-03-27T13:51:03.342832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83ff9425",
   "metadata": {
    "papermill": {
     "duration": 0.049275,
     "end_time": "2023-03-27T13:51:03.469275",
     "exception": false,
     "start_time": "2023-03-27T13:51:03.420000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=config[\"N_EPOCHS\"], warm_method='log'):\n",
    "    \n",
    "    if current_step < num_warmup_steps:\n",
    "        if warm_method == 'log':\n",
    "            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n",
    "        else:\n",
    "            return lr_max * 2 ** -(num_warmup_steps - current_step)\n",
    "    else:\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efbb6b24",
   "metadata": {
    "papermill": {
     "duration": 0.738209,
     "end_time": "2023-03-27T13:51:04.245805",
     "exception": false,
     "start_time": "2023-03-27T13:51:03.507596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning rate for encoder\n",
    "LR_SCHEDULE = [lrfn(step, num_warmup_steps=config[\"N_WARMUP_EPOCHS\"], \n",
    "                    lr_max=config[\"LEARNING_RATE\"], num_cycles=0.50) for step in range(config[\"N_EPOCHS\"])]\n",
    "\n",
    "# Learning Rate Callback\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f49ca",
   "metadata": {
    "papermill": {
     "duration": 0.040725,
     "end_time": "2023-03-27T13:51:04.330003",
     "exception": false,
     "start_time": "2023-03-27T13:51:04.289278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weight Decay Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef2cbe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom callback to update weight decay with learning rate\n",
    "class WeightDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, wd_ratio=config['WD_RATIO']):\n",
    "        self.step_counter = 0\n",
    "        self.wd_ratio = wd_ratio\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n",
    "        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac727612",
   "metadata": {
    "papermill": {
     "duration": 0.040685,
     "end_time": "2023-03-27T13:51:11.352812",
     "exception": false,
     "start_time": "2023-03-27T13:51:11.312127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb34e2f",
   "metadata": {
    "papermill": {
     "duration": 2852.852405,
     "end_time": "2023-03-27T14:38:44.682355",
     "exception": false,
     "start_time": "2023-03-27T13:51:11.829950",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " NON_EMPTY_FRAME_IDXS (InputLay  [(None, 38)]        0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " FRAMES (InputLayer)            [(None, 38, 454)]    0           []                               \n",
      "                                                                                                  \n",
      " tf.math.not_equal (TFOpLambda)  (None, 38)          0           ['NON_EMPTY_FRAME_IDXS[0][0]']   \n",
      "                                                                                                  \n",
      " tf.slice (TFOpLambda)          (None, 38, 320)      0           ['FRAMES[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.slice_1 (TFOpLambda)        (None, 38, 42)       0           ['FRAMES[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.slice_3 (TFOpLambda)        (None, 38, 42)       0           ['FRAMES[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.slice_2 (TFOpLambda)        (None, 38, 50)       0           ['FRAMES[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None, 38)           0           ['tf.math.not_equal[0][0]']      \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 38, 512)      1244420     ['tf.slice[0][0]',               \n",
      "                                                                  'tf.slice_1[0][0]',             \n",
      "                                                                  'tf.slice_3[0][0]',             \n",
      "                                                                  'tf.slice_2[0][0]',             \n",
      "                                                                  'NON_EMPTY_FRAME_IDXS[0][0]']   \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 38, 1)        0           ['tf.cast[0][0]']                \n",
      "                                                                                                  \n",
      " transformer (Transformer)      (None, 38, 512)      8411136     ['embedding[0][0]',              \n",
      "                                                                  'tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 38, 512)      0           ['transformer[0][0]',            \n",
      "                                                                  'tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (None, 512)         0           ['tf.math.multiply[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOpLamb  (None, 1)           0           ['tf.expand_dims[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambda)   (None, 512)          0           ['tf.math.reduce_sum[0][0]',     \n",
      "                                                                  'tf.math.reduce_sum_1[0][0]']   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 250)          128250      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,783,806\n",
      "Trainable params: 9,783,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.01.\n",
      "learning rate: 1.00e-02, weight decay: 5.00e-04\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 06:43:17.429632: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/590 [==========>...................] - ETA: 1:34 - loss: 86.3063 - acc: 0.0118 - top_5_acc: 0.0652 - top_10_acc: 0.1449"
     ]
    }
   ],
   "source": [
    "# Clear all models in GPU\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Get new fresh model\n",
    "model = get_model()\n",
    "\n",
    "# Sanity Check\n",
    "model.summary()\n",
    "\n",
    "log_dir = \"./logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Actual Training\n",
    "history = model.fit(\n",
    "        x=get_train_batch(X_train, y_train, NON_EMPTY_FRAME_IDXS_train, batch_size=config['TRAIN_BATCH_SIZE']),\n",
    "        steps_per_epoch= len(X_train) // config['TRAIN_BATCH_SIZE'],\n",
    "        epochs=config[\"N_EPOCHS\"],\n",
    "        validation_data= ({ 'frames': X_val, 'non_empty_frame_idxs': NON_EMPTY_FRAME_IDXS_val }, y_val),\n",
    "        batch_size=config[\"BATCH_SIZE_VAL\"],\n",
    "        callbacks=[\n",
    "            lr_callback,\n",
    "            WeightDecayCallback(config[\"WD_RATIO\"]),\n",
    "#             tensorboard_callback\n",
    "          ],\n",
    "        verbose = VERBOSE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78674927",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-19T05:50:20.628656Z",
     "iopub.status.idle": "2023-04-19T05:50:20.629604Z",
     "shell.execute_reply": "2023-04-19T05:50:20.629353Z",
     "shell.execute_reply.started": "2023-04-19T05:50:20.629322Z"
    },
    "papermill": {
     "duration": 0.204275,
     "end_time": "2023-03-27T14:38:44.950811",
     "exception": false,
     "start_time": "2023-03-27T14:38:44.746536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Model Weights\n",
    "model.save_weights(f'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf980c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-19T05:50:20.631264Z",
     "iopub.status.idle": "2023-04-19T05:50:20.631765Z",
     "shell.execute_reply": "2023-04-19T05:50:20.631531Z",
     "shell.execute_reply.started": "2023-04-19T05:50:20.631505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# model.load_weights('tf_models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0df4e",
   "metadata": {
    "papermill": {
     "duration": 0.065813,
     "end_time": "2023-03-27T14:38:48.284324",
     "exception": false,
     "start_time": "2023-03-27T14:38:48.218511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission\n",
    "\n",
    "Submission code loosley based on [this notebook](https://www.kaggle.com/code/dschettler8845/gislr-learn-eda-baseline#baseline) by [Darien Schettler\n",
    "](https://www.kaggle.com/dschettler8845)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ce3fe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-19T05:50:20.633657Z",
     "iopub.status.idle": "2023-04-19T05:50:20.634182Z",
     "shell.execute_reply": "2023-04-19T05:50:20.633929Z",
     "shell.execute_reply.started": "2023-04-19T05:50:20.633901Z"
    },
    "papermill": {
     "duration": 1.644953,
     "end_time": "2023-03-27T14:38:49.995113",
     "exception": false,
     "start_time": "2023-03-27T14:38:48.350160",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TFLite model for submission\n",
    "class TFLiteModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "\n",
    "        # Load the feature generation and main models\n",
    "        self.preprocess_layer = preprocess_layer\n",
    "        self.model = model\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, config[\"N_ROWS\"], config[\"N_DIMS\"]], dtype=tf.float32, name='inputs')])\n",
    "    def call(self, inputs):\n",
    "        # Preprocess Data\n",
    "        x, non_empty_frame_idxs = self.preprocess_layer(inputs)\n",
    "        # Add Batch Dimension\n",
    "        x = tf.expand_dims(x, axis=0)\n",
    "        non_empty_frame_idxs = tf.expand_dims(non_empty_frame_idxs, axis=0)\n",
    "        # Make Prediction\n",
    "        outputs = self.model({'FRAMES': x, 'NON_EMPTY_FRAME_IDXS': non_empty_frame_idxs })\n",
    "        # Squeeze Output 1x250 -> 250\n",
    "        outputs = tf.squeeze(outputs, axis=0)\n",
    "\n",
    "        # Return a dictionary with the output tensor\n",
    "        return {'outputs': outputs}\n",
    "\n",
    "# Define TF Lite Model\n",
    "tflite_keras_model = TFLiteModel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cf4b6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-19T05:50:20.635956Z",
     "iopub.status.idle": "2023-04-19T05:50:20.636481Z",
     "shell.execute_reply": "2023-04-19T05:50:20.636246Z",
     "shell.execute_reply.started": "2023-04-19T05:50:20.636210Z"
    },
    "papermill": {
     "duration": 32.570402,
     "end_time": "2023-03-27T14:39:22.633212",
     "exception": false,
     "start_time": "2023-03-27T14:38:50.062810",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Model Converter\n",
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflite_keras_model)\n",
    "keras_model_converter.experimental_new_converter = True\n",
    "\n",
    "# Convert Model\n",
    "tflite_model = keras_model_converter.convert()\n",
    "# Write Model\n",
    "with open(f'model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Zip Model\n",
    "!zip submission.zip /kaggle/working/model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15fede1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-19T05:50:20.638302Z",
     "iopub.status.idle": "2023-04-19T05:50:20.638805Z",
     "shell.execute_reply": "2023-04-19T05:50:20.638569Z",
     "shell.execute_reply.started": "2023-04-19T05:50:20.638543Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "demo_raw_data = load_relevant_data_subset(train_metadata['file_path'].values[2])\n",
    "print(f'demo_raw_data shape: {demo_raw_data.shape}, dtype: {demo_raw_data.dtype}')\n",
    "demo_output = tflite_keras_model(demo_raw_data)[\"outputs\"]\n",
    "print(f'demo_output shape: {demo_output.shape}, dtype: {demo_output.dtype}')\n",
    "demo_prediction = demo_output.numpy().argmax()\n",
    "print(f'demo_prediction: {demo_prediction}, correct: {train_metadata.iloc[0][\"sign_ord\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b01c0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-19T05:50:20.640611Z",
     "iopub.status.idle": "2023-04-19T05:50:20.641138Z",
     "shell.execute_reply": "2023-04-19T05:50:20.640885Z",
     "shell.execute_reply.started": "2023-04-19T05:50:20.640857Z"
    },
    "papermill": {
     "duration": 11.438209,
     "end_time": "2023-03-27T14:39:34.137659",
     "exception": false,
     "start_time": "2023-03-27T14:39:22.699450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify TFLite model can be loaded and used for prediction\n",
    "# !pip install tflite-runtime\n",
    "# import tf.lite.interpreter as tflite\n",
    "\n",
    "interpreter = tf.lite.Interpreter(\"/kaggle/working/model.tflite\")\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "output = prediction_fn(inputs=demo_raw_data)\n",
    "sign = output['outputs'].argmax()\n",
    "\n",
    "print(\"PRED : \", ORD2SIGN.get(sign), f'[{sign}]')\n",
    "print(\"TRUE : \", train_metadata.sign.values[0], f'[{train_metadata.sign_ord.values[0]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346b1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3038.388613,
   "end_time": "2023-03-27T14:39:37.370611",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-27T13:48:58.981998",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03d6f570ff764870bb73ccfb8c6887af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_49350bdfb80d4f00a698d195ad6cc3e3",
       "placeholder": "",
       "style": "IPY_MODEL_6114b828351443dead1df1f805e5cfd7",
       "value": "100%"
      }
     },
     "07287dd36a6b457bab873107ef3a9a74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a3cb9fa235b4d3b9fe4729d0488ab41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0b6a231f62a24aae9fa0cbc3fb92820f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0bdd95bee39a4c0fba0d526a76e11ed7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5809fb0e32f44de489c4ffd133aed91d",
       "placeholder": "",
       "style": "IPY_MODEL_ac638cb770cd4789a3ad5944574c0685",
       "value": " 1000/1000 [00:23&lt;00:00, 37.18it/s]"
      }
     },
     "0f47d306ae4c4968ae9578b5d3f96717": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1c0862516d904ef7a8308a72e5ae9f91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28320ac6793c47f69b05122f608c2d44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d0ef2e7de83043f5a1e56b8cbfdceb42",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0b6a231f62a24aae9fa0cbc3fb92820f",
       "value": 1000
      }
     },
     "37eb2c935499475c89e659645e41c052": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3a30e5dd2b7e4e58b5f796dc56795f9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "43441f82927941c183c36d4ba42c6839": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49104b8e9766498997f1c107709166cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49350bdfb80d4f00a698d195ad6cc3e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4fb37e6c0a6849b0ae4fca96c98079d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50b095287b764b40bc01eecfb8e226ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "549182ad731149e196c24e0c65368459": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "54d833de2b3e4368a09e8f2832ae582a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_43441f82927941c183c36d4ba42c6839",
       "placeholder": "",
       "style": "IPY_MODEL_549182ad731149e196c24e0c65368459",
       "value": " 42/42 [00:06&lt;00:00,  6.63it/s]"
      }
     },
     "5809fb0e32f44de489c4ffd133aed91d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b6ddc597476439b98e2a8572db415ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5f60bd20e6054b8d9990b2b2804e19a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6114b828351443dead1df1f805e5cfd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7a008f4929ca442296cb2159b02d1206": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "873e173a1a48496183ab1a8b2425b3f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89aab12028bb44f0a421c39e5407e2c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a757d761f004d7faf0e86f35c7cc2ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d56ea444a554a8d86255febcfa438ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f60bd20e6054b8d9990b2b2804e19a1",
       "placeholder": "",
       "style": "IPY_MODEL_37eb2c935499475c89e659645e41c052",
       "value": "100%"
      }
     },
     "8e1a02523adf4a51928bd01fe6a016a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a1b2e1be36c74ff5802bee0472d05a8c",
        "IPY_MODEL_974821174ce548f7ab78f2eb280a36bc",
        "IPY_MODEL_54d833de2b3e4368a09e8f2832ae582a"
       ],
       "layout": "IPY_MODEL_49104b8e9766498997f1c107709166cd"
      }
     },
     "974821174ce548f7ab78f2eb280a36bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4fb37e6c0a6849b0ae4fca96c98079d3",
       "max": 42,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0a3cb9fa235b4d3b9fe4729d0488ab41",
       "value": 42
      }
     },
     "97ebc5f11183491097eaa468c8f8d6ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9fdf9f1a806d45d8afc6aa7674799d7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8d56ea444a554a8d86255febcfa438ef",
        "IPY_MODEL_bc93196873734683876ec5f8b4b4c62c",
        "IPY_MODEL_d34a43c029834695b633aa4d4d941930"
       ],
       "layout": "IPY_MODEL_873e173a1a48496183ab1a8b2425b3f1"
      }
     },
     "a1b2e1be36c74ff5802bee0472d05a8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_07287dd36a6b457bab873107ef3a9a74",
       "placeholder": "",
       "style": "IPY_MODEL_fc0e155d7357432c9976cb2ac7c68b3a",
       "value": "100%"
      }
     },
     "ac638cb770cd4789a3ad5944574c0685": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b0ba62ab3b254a6897d45bb7a753f044": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b5005dde82d843e7a0b50cdf2669d881": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d7996506467449ad9ba51cfb1e6db002",
        "IPY_MODEL_d0ff43d2109d482884de8add17da97e7",
        "IPY_MODEL_f9647d54b313481eaa72a3093a5dbc0a"
       ],
       "layout": "IPY_MODEL_1c0862516d904ef7a8308a72e5ae9f91"
      }
     },
     "bc93196873734683876ec5f8b4b4c62c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fa12da626fe2424eb7a18e996ed1fb96",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5b6ddc597476439b98e2a8572db415ee",
       "value": 10
      }
     },
     "d0ef2e7de83043f5a1e56b8cbfdceb42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0ff43d2109d482884de8add17da97e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a30e5dd2b7e4e58b5f796dc56795f9d",
       "max": 40,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_97ebc5f11183491097eaa468c8f8d6ba",
       "value": 40
      }
     },
     "d34a43c029834695b633aa4d4d941930": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_50b095287b764b40bc01eecfb8e226ce",
       "placeholder": "",
       "style": "IPY_MODEL_7a008f4929ca442296cb2159b02d1206",
       "value": " 10/10 [00:02&lt;00:00,  5.02it/s]"
      }
     },
     "d7996506467449ad9ba51cfb1e6db002": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e8ec2cab76c64079add86c4cd9e47463",
       "placeholder": "",
       "style": "IPY_MODEL_0f47d306ae4c4968ae9578b5d3f96717",
       "value": "100%"
      }
     },
     "e8ec2cab76c64079add86c4cd9e47463": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "edea63966eb74395b1ccf14ba9265099": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_03d6f570ff764870bb73ccfb8c6887af",
        "IPY_MODEL_28320ac6793c47f69b05122f608c2d44",
        "IPY_MODEL_0bdd95bee39a4c0fba0d526a76e11ed7"
       ],
       "layout": "IPY_MODEL_89aab12028bb44f0a421c39e5407e2c2"
      }
     },
     "f9647d54b313481eaa72a3093a5dbc0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8a757d761f004d7faf0e86f35c7cc2ef",
       "placeholder": "",
       "style": "IPY_MODEL_b0ba62ab3b254a6897d45bb7a753f044",
       "value": " 40/40 [00:07&lt;00:00,  4.48it/s]"
      }
     },
     "fa12da626fe2424eb7a18e996ed1fb96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc0e155d7357432c9976cb2ac7c68b3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
